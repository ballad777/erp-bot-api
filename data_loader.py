import os
import re
import time
import json
import logging
from datetime import datetime
from typing import Dict, Any, Optional, List
from io import BytesIO

import requests
import pandas as pd
from sqlalchemy import create_engine, text, inspect
from sqlalchemy.engine import Engine

# =========================
# Logging
# =========================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("data_loader")

# =========================
# Env
# =========================
DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./erp.db")
if DATABASE_URL.startswith("postgres://"):
    DATABASE_URL = DATABASE_URL.replace("postgres://", "postgresql://", 1)

# Google Sheet URL
SALES_SHEET_URL = os.getenv("SALES_EXCEL_URL", "").strip()
PURCHASE_SHEET_URL = os.getenv("PURCHASE_EXCEL_URL", "").strip()

engine: Engine = create_engine(DATABASE_URL, pool_pre_ping=True, future=True)


# =========================
# DB init
# =========================
def ensure_tables_and_indexes() -> None:
    """
    1) ç¢ºä¿è³‡æ–™è¡¨å­˜åœ¨
    2) å˜—è©¦å»ºç«‹ç´¢å¼•
    3) å¦‚æœæœ‰é‡è¤‡è³‡æ–™å°è‡´ç´¢å¼•å¤±æ•—ï¼Œè‡ªå‹•å»é‡å¾Œé‡è©¦
    """
    logger.info("ğŸ”§ æª¢æŸ¥è³‡æ–™è¡¨å’Œç´¢å¼•...")
    dialect = engine.url.get_backend_name()

    with engine.begin() as conn:
        # å»ºç«‹ sales è¡¨
        conn.execute(text("""
        CREATE TABLE IF NOT EXISTS sales (
            date TEXT,
            customer TEXT,
            product TEXT,
            quantity REAL,
            amount REAL,
            year INTEGER
        );
        """))
        
        # å»ºç«‹ purchase è¡¨
        conn.execute(text("""
        CREATE TABLE IF NOT EXISTS purchase (
            date TEXT,
            supplier TEXT,
            product TEXT,
            quantity REAL,
            amount REAL,
            year INTEGER
        );
        """))

        def dedup_postgres(table: str, cols: List[str]) -> None:
            """PostgreSQL å»é‡"""
            cond = " AND ".join([f"a.{c} = b.{c}" for c in cols])
            sql = f"""
            DELETE FROM {table} a
            USING {table} b
            WHERE {cond}
              AND a.ctid > b.ctid;
            """
            conn.execute(text(sql))

        def dedup_sqlite(table: str, cols: List[str]) -> None:
            """SQLite å»é‡"""
            group_by = ", ".join(cols)
            sql = f"""
            DELETE FROM {table}
            WHERE rowid NOT IN (
                SELECT MIN(rowid)
                FROM {table}
                GROUP BY {group_by}
            );
            """
            conn.execute(text(sql))

        def create_unique_index(table: str, index_name: str, cols: List[str]) -> None:
            """å»ºç«‹å”¯ä¸€ç´¢å¼•"""
            cols_join = ", ".join(cols)
            conn.execute(text(f"""
            CREATE UNIQUE INDEX IF NOT EXISTS {index_name}
            ON {table}({cols_join});
            """))

        # è™•ç† sales ç´¢å¼•
        try:
            create_unique_index("sales", "ux_sales_row", 
                              ["date", "customer", "product", "amount", "quantity"])
            logger.info("âœ… sales ç´¢å¼•å·²å»ºç«‹")
        except Exception as e:
            logger.warning(f"âš ï¸ sales ç´¢å¼•å»ºç«‹å¤±æ•—ï¼Œå˜—è©¦å»é‡: {e}")
            try:
                if dialect == "postgresql":
                    dedup_postgres("sales", ["date", "customer", "product", "amount", "quantity"])
                else:
                    dedup_sqlite("sales", ["date", "customer", "product", "amount", "quantity"])
                create_unique_index("sales", "ux_sales_row", 
                                  ["date", "customer", "product", "amount", "quantity"])
                logger.info("âœ… sales å»é‡å®Œæˆï¼Œç´¢å¼•å·²å»ºç«‹")
            except Exception as e2:
                logger.error(f"âŒ sales ç´¢å¼•ä»ç„¶å¤±æ•—ï¼Œç¹¼çºŒåŸ·è¡Œ: {e2}")

        # è™•ç† purchase ç´¢å¼•
        try:
            create_unique_index("purchase", "ux_purchase_row", 
                              ["date", "supplier", "product", "amount", "quantity"])
            logger.info("âœ… purchase ç´¢å¼•å·²å»ºç«‹")
        except Exception as e:
            logger.warning(f"âš ï¸ purchase ç´¢å¼•å»ºç«‹å¤±æ•—ï¼Œå˜—è©¦å»é‡: {e}")
            try:
                if dialect == "postgresql":
                    dedup_postgres("purchase", ["date", "supplier", "product", "amount", "quantity"])
                else:
                    dedup_sqlite("purchase", ["date", "supplier", "product", "amount", "quantity"])
                create_unique_index("purchase", "ux_purchase_row", 
                                  ["date", "supplier", "product", "amount", "quantity"])
                logger.info("âœ… purchase å»é‡å®Œæˆï¼Œç´¢å¼•å·²å»ºç«‹")
            except Exception as e2:
                logger.error(f"âŒ purchase ç´¢å¼•ä»ç„¶å¤±æ•—ï¼Œç¹¼çºŒåŸ·è¡Œ: {e2}")


def table_counts() -> Dict[str, int]:
    """å–å¾—å„è¡¨ç­†æ•¸"""
    insp = inspect(engine)
    names = set(insp.get_table_names())
    out = {"sales": 0, "purchase": 0}
    
    with engine.connect() as conn:
        for t in out.keys():
            if t in names:
                try:
                    result = conn.execute(text(f"SELECT COUNT(*) FROM {t}"))
                    out[t] = int(result.scalar() or 0)
                except:
                    out[t] = 0
    
    return out


# =========================
# Google Sheet ä¸‹è¼‰
# =========================
def extract_sheet_id(url: str) -> Optional[str]:
    """å¾ URL æå– Sheet ID"""
    patterns = [
        r"/spreadsheets/d/([a-zA-Z0-9_-]+)",
        r"id=([a-zA-Z0-9_-]+)",
    ]
    for pattern in patterns:
        m = re.search(pattern, url)
        if m:
            return m.group(1)
    return None


def download_google_sheet_xlsx(sheet_url: str, max_retries: int = 3) -> Optional[BytesIO]:
    """
    ä¸‹è¼‰ Google Sheet ç‚º Excel æ ¼å¼
    å›å‚³ BytesIO æˆ– None
    """
    sheet_id = extract_sheet_id(sheet_url)
    if not sheet_id:
        logger.error(f"âŒ ç„¡æ³•æå– Sheet ID: {sheet_url}")
        return None

    export_url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/export"
    params = {"format": "xlsx"}
    
    logger.info(f"ğŸ“¥ ä¸‹è¼‰ Google Sheet: {sheet_id}")

    for attempt in range(max_retries):
        try:
            response = requests.get(
                export_url,
                params=params,
                timeout=60,
                allow_redirects=True
            )
            
            if response.status_code != 200:
                logger.error(f"âŒ HTTP {response.status_code}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                return None

            content = response.content
            
            # æª¢æŸ¥æ˜¯å¦ç‚º Excel (ZIP æ ¼å¼ï¼Œä»¥ PK é–‹é ­)
            if not content.startswith(b'PK'):
                logger.error("âŒ å›æ‡‰ä¸æ˜¯ Excel æ ¼å¼")
                
                # æª¢æŸ¥æ˜¯å¦ç‚º HTML (æ¬Šé™å•é¡Œ)
                if b'<html' in content[:500].lower():
                    logger.error("âŒ æ”¶åˆ° HTMLï¼Œå¯èƒ½æ˜¯æ¬Šé™å•é¡Œ")
                    logger.error("è«‹ç¢ºèª Google Sheet å·²è¨­ç‚ºã€ŒçŸ¥é“é€£çµçš„äººå¯ä»¥æª¢è¦–ã€")
                    logger.error(f"å‰ 200 å­—å…ƒ: {content[:200]}")
                
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                return None
            
            logger.info(f"âœ… ä¸‹è¼‰æˆåŠŸ: {len(content)} bytes")
            return BytesIO(content)

        except requests.exceptions.Timeout:
            logger.error(f"â±ï¸ ä¸‹è¼‰è¶…æ™‚ (å˜—è©¦ {attempt + 1}/{max_retries})")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
        except Exception as e:
            logger.error(f"âŒ ä¸‹è¼‰éŒ¯èª¤: {str(e)}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)

    return None


# =========================
# æ¬„ä½åŒ¹é…è¼”åŠ©å‡½æ•¸
# =========================
def find_column(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:
    """å¾å€™é¸æ¬„ä½åç¨±ä¸­æ‰¾åˆ°ç¬¬ä¸€å€‹å­˜åœ¨çš„"""
    df.columns = df.columns.astype(str).str.strip()
    for candidate in candidates:
        for col in df.columns:
            if candidate in col:
                return col
    return None


# =========================
# æ¨™æº–åŒ–å‡½æ•¸ï¼ˆå½ˆæ€§ç‰ˆæœ¬ï¼‰
# =========================
def normalize_sales_df(df: pd.DataFrame) -> Optional[pd.DataFrame]:
    """æ¨™æº–åŒ–éŠ·å”®è³‡æ–™ï¼ˆæ”¯æ´å¤šç¨®æ¬„ä½åç¨±ï¼‰"""
    logger.info(f"è™•ç†éŠ·å”®è³‡æ–™ï¼Œæ¬„ä½: {list(df.columns)}")
    
    # å½ˆæ€§å°‹æ‰¾æ¬„ä½
    date_col = find_column(df, ["æ—¥æœŸ(è½‰æ›)", "æ—¥æœŸ", "Date", "date", "äº¤æ˜“æ—¥æœŸ"])
    customer_col = find_column(df, ["å®¢æˆ¶ä¾›æ‡‰å•†ç°¡ç¨±", "å®¢æˆ¶ç°¡ç¨±", "å®¢æˆ¶", "Customer", "å®¢æˆ¶åç¨±"])
    product_col = find_column(df, ["å“å", "ç”¢å“", "Product", "å“è™Ÿ", "ç”¢å“ä»£è™Ÿ"])
    quantity_col = find_column(df, ["æ•¸é‡", "Quantity", "éŠ·å”®æ•¸é‡"])
    amount_col = find_column(df, ["é€²éŠ·æ˜ç´°æœªç¨…é‡‘é¡", "æœªç¨…é‡‘é¡", "é‡‘é¡", "Amount"])

    # æª¢æŸ¥å¿…è¦æ¬„ä½
    if not all([date_col, customer_col, product_col]):
        logger.warning(f"âš ï¸ ç¼ºå°‘å¿…è¦æ¬„ä½: date={date_col}, customer={customer_col}, product={product_col}")
        return None

    # å»ºç«‹æ¨™æº–åŒ– DataFrame
    clean = pd.DataFrame({
        "date": pd.to_datetime(df[date_col], errors="coerce"),
        "customer": df[customer_col].astype(str).str.strip(),
        "product": df[product_col].astype(str).str.strip(),
        "quantity": pd.to_numeric(df[quantity_col], errors="coerce").fillna(0) if quantity_col else 0,
        "amount": pd.to_numeric(df[amount_col], errors="coerce").fillna(0) if amount_col else 0,
    }).dropna(subset=["date"])

    clean["year"] = clean["date"].dt.year
    clean["date"] = clean["date"].dt.strftime("%Y-%m-%d")
    
    logger.info(f"âœ… æ¨™æº–åŒ–å®Œæˆ: {len(clean)} ç­†")
    return clean


def normalize_purchase_df(df: pd.DataFrame) -> Optional[pd.DataFrame]:
    """æ¨™æº–åŒ–æ¡è³¼è³‡æ–™ï¼ˆæ”¯æ´å¤šç¨®æ¬„ä½åç¨±ï¼‰"""
    logger.info(f"è™•ç†æ¡è³¼è³‡æ–™ï¼Œæ¬„ä½: {list(df.columns)}")
    
    # å½ˆæ€§å°‹æ‰¾æ¬„ä½
    date_col = find_column(df, ["æ—¥æœŸ(è½‰æ›)", "æ—¥æœŸ", "Date", "date", "äº¤æ˜“æ—¥æœŸ"])
    supplier_col = find_column(df, ["å®¢æˆ¶ä¾›æ‡‰å•†ç°¡ç¨±", "ä¾›æ‡‰å•†", "Supplier", "å» å•†"])
    product_col = find_column(df, ["å°æ–¹å“å/å“åå‚™è¨»", "å“å", "ç”¢å“", "Product", "å“è™Ÿ"])
    quantity_col = find_column(df, ["æ•¸é‡", "Quantity", "æ¡è³¼æ•¸é‡"])
    amount_col = find_column(df, ["é€²éŠ·æ˜ç´°æœªç¨…é‡‘é¡", "æœªç¨…é‡‘é¡", "é‡‘é¡", "Amount"])

    # æª¢æŸ¥å¿…è¦æ¬„ä½
    if not all([date_col, supplier_col, product_col]):
        logger.warning(f"âš ï¸ ç¼ºå°‘å¿…è¦æ¬„ä½: date={date_col}, supplier={supplier_col}, product={product_col}")
        return None

    # å»ºç«‹æ¨™æº–åŒ– DataFrame
    clean = pd.DataFrame({
        "date": pd.to_datetime(df[date_col], errors="coerce"),
        "supplier": df[supplier_col].astype(str).str.strip(),
        "product": df[product_col].astype(str).str.strip(),
        "quantity": pd.to_numeric(df[quantity_col], errors="coerce").fillna(0) if quantity_col else 0,
        "amount": pd.to_numeric(df[amount_col], errors="coerce").fillna(0) if amount_col else 0,
    }).dropna(subset=["date"])

    clean["year"] = clean["date"].dt.year
    clean["date"] = clean["date"].dt.strftime("%Y-%m-%d")
    
    logger.info(f"âœ… æ¨™æº–åŒ–å®Œæˆ: {len(clean)} ç­†")
    return clean


# =========================
# æ’å…¥è³‡æ–™ï¼ˆå¿½ç•¥é‡è¤‡ï¼‰
# =========================
def insert_ignore(kind: str, df: pd.DataFrame) -> int:
    """
    æ’å…¥è³‡æ–™ï¼Œé‡è¤‡çš„æœƒè‡ªå‹•å¿½ç•¥
    å›å‚³å˜—è©¦æ’å…¥çš„ç­†æ•¸
    """
    if df.empty:
        return 0

    dialect = engine.url.get_backend_name()
    rows = df.to_dict(orient="records")

    with engine.begin() as conn:
        if kind == "sales":
            if dialect == "postgresql":
                stmt = text("""
                INSERT INTO sales(date, customer, product, quantity, amount, year)
                VALUES (:date, :customer, :product, :quantity, :amount, :year)
                ON CONFLICT (date, customer, product, amount, quantity) DO NOTHING;
                """)
            else:
                stmt = text("""
                INSERT OR IGNORE INTO sales(date, customer, product, quantity, amount, year)
                VALUES (:date, :customer, :product, :quantity, :amount, :year);
                """)
        else:  # purchase
            if dialect == "postgresql":
                stmt = text("""
                INSERT INTO purchase(date, supplier, product, quantity, amount, year)
                VALUES (:date, :supplier, :product, :quantity, :amount, :year)
                ON CONFLICT (date, supplier, product, amount, quantity) DO NOTHING;
                """)
            else:
                stmt = text("""
                INSERT OR IGNORE INTO purchase(date, supplier, product, quantity, amount, year)
                VALUES (:date, :supplier, :product, :quantity, :amount, :year);
                """)

        for r in rows:
            conn.execute(stmt, r)

    return len(rows)


# =========================
# ä¸»è¦åŒ¯å…¥å‡½æ•¸
# =========================
def import_from_sheets() -> Dict[str, Any]:
    """å¾ Google Sheets åŒ¯å…¥è³‡æ–™"""
    logger.info("ğŸ”„ é–‹å§‹è³‡æ–™åŒ¯å…¥...")
    
    ensure_tables_and_indexes()

    before = table_counts()
    messages: List[str] = []

    # åŒ¯å…¥éŠ·å”®è³‡æ–™
    if SALES_SHEET_URL:
        logger.info("ğŸ“Š è™•ç†éŠ·å”®è³‡æ–™...")
        excel_bytes = download_google_sheet_xlsx(SALES_SHEET_URL)
        
        if excel_bytes:
            try:
                xls = pd.read_excel(excel_bytes, sheet_name=None)
                logger.info(f"æ‰¾åˆ° {len(xls)} å€‹å·¥ä½œè¡¨")
                
                parts = []
                for sheet_name, df in xls.items():
                    logger.info(f"è™•ç†å·¥ä½œè¡¨: {sheet_name}")
                    normalized = normalize_sales_df(df)
                    if normalized is not None and not normalized.empty:
                        parts.append(normalized)
                
                if parts:
                    final = pd.concat(parts, ignore_index=True)
                    insert_ignore("sales", final)
                    messages.append(f"âœ… sales: è®€åˆ° {len(final)} ç­†ï¼ˆå¢é‡åŒ¯å…¥ï¼Œé‡è¤‡æœƒè‡ªå‹•ç•¥éï¼‰")
                else:
                    messages.append("âš ï¸ sales: æ²’æ‰¾åˆ°ç¬¦åˆæ¬„ä½çš„åˆ†é ")
            except Exception as e:
                logger.error(f"âŒ sales è™•ç†éŒ¯èª¤: {str(e)}", exc_info=True)
                messages.append(f"âŒ sales: è™•ç†å¤±æ•— - {str(e)}")
        else:
            messages.append("âŒ sales: ä¸‹è¼‰å¤±æ•—ï¼ˆè«‹ç¢ºèª Google Sheet å·²è¨­ç‚ºã€ŒçŸ¥é“é€£çµçš„äººå¯æª¢è¦–ã€ï¼‰")
    else:
        messages.append("â„¹ï¸ sales: æœªè¨­å®š SALES_EXCEL_URL")

    # åŒ¯å…¥æ¡è³¼è³‡æ–™
    if PURCHASE_SHEET_URL:
        logger.info("ğŸ“¦ è™•ç†æ¡è³¼è³‡æ–™...")
        excel_bytes = download_google_sheet_xlsx(PURCHASE_SHEET_URL)
        
        if excel_bytes:
            try:
                xls = pd.read_excel(excel_bytes, sheet_name=None)
                logger.info(f"æ‰¾åˆ° {len(xls)} å€‹å·¥ä½œè¡¨")
                
                parts = []
                for sheet_name, df in xls.items():
                    logger.info(f"è™•ç†å·¥ä½œè¡¨: {sheet_name}")
                    normalized = normalize_purchase_df(df)
                    if normalized is not None and not normalized.empty:
                        parts.append(normalized)
                
                if parts:
                    final = pd.concat(parts, ignore_index=True)
                    insert_ignore("purchase", final)
                    messages.append(f"âœ… purchase: è®€åˆ° {len(final)} ç­†ï¼ˆå¢é‡åŒ¯å…¥ï¼Œé‡è¤‡æœƒè‡ªå‹•ç•¥éï¼‰")
                else:
                    messages.append("âš ï¸ purchase: æ²’æ‰¾åˆ°ç¬¦åˆæ¬„ä½çš„åˆ†é ")
            except Exception as e:
                logger.error(f"âŒ purchase è™•ç†éŒ¯èª¤: {str(e)}", exc_info=True)
                messages.append(f"âŒ purchase: è™•ç†å¤±æ•— - {str(e)}")
        else:
            messages.append("âŒ purchase: ä¸‹è¼‰å¤±æ•—ï¼ˆè«‹ç¢ºèª Google Sheet å·²è¨­ç‚ºã€ŒçŸ¥é“é€£çµçš„äººå¯æª¢è¦–ã€ï¼‰")
    else:
        messages.append("â„¹ï¸ purchase: æœªè¨­å®š PURCHASE_EXCEL_URL")

    after = table_counts()
    
    result = {
        "ok": True,
        "before": before,
        "after": after,
        "messages": messages,
        "db": engine.url.get_backend_name(),
        "time": datetime.utcnow().isoformat() + "Z",
    }
    
    logger.info(f"âœ¨ è³‡æ–™åŒ¯å…¥å®Œæˆ: {result}")
    return result


if __name__ == "__main__":
    result = import_from_sheets()
    print(json.dumps(result, ensure_ascii=False, indent=2))
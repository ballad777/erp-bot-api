import os
import re
import json
import time
import hmac
import base64
import hashlib
import logging
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from io import BytesIO

import requests
import pandas as pd
import httpx

from fastapi import FastAPI, Request, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse, Response

from sqlalchemy import create_engine, text, inspect
from sqlalchemy.engine import Engine

from google import genai
from google.genai import types

import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt

# =========================================================
# Logging
# =========================================================
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("erp_ultra_pro")

# =========================================================
# App
# =========================================================
app = FastAPI(title="ERP Bot Ultra PRO", version="3.2_Fixed")

# =========================================================
# Environment
# =========================================================
DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///./erp.db")
if DATABASE_URL.startswith("postgres://"):
    DATABASE_URL = DATABASE_URL.replace("postgres://", "postgresql://", 1)

LINE_CHANNEL_ACCESS_TOKEN = os.getenv("LINE_CHANNEL_ACCESS_TOKEN", "")
LINE_CHANNEL_SECRET = os.getenv("LINE_CHANNEL_SECRET", "")

GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
MODEL_NAME = os.getenv("GEMINI_MODEL", "gemini-2.0-flash-exp")

# æ”¯æ´å¤šç¨® URL æ ¼å¼
SALES_SHEET_URL = os.getenv("SALES_EXCEL_URL", "")
PURCHASE_SHEET_URL = os.getenv("PURCHASE_EXCEL_URL", "")

ADMIN_TOKEN = os.getenv("ADMIN_TOKEN", "secret123")
AUTO_IMPORT_ON_STARTUP = os.getenv("AUTO_IMPORT_ON_STARTUP", "0") == "1"

RATE_LIMIT_PER_MIN = int(os.getenv("RATE_LIMIT_PER_MIN", "40"))

# =========================================================
# Globals
# =========================================================
engine: Engine = create_engine(DATABASE_URL, pool_pre_ping=True, future=True)
client = genai.Client(api_key=GEMINI_API_KEY) if GEMINI_API_KEY else None

RATE_STORE: Dict[str, List[float]] = {}
CHAT_MEMORY: Dict[str, List[Any]] = {}
IMG_STORE: Dict[str, Dict[str, Any]] = {}
DB_READY = False

# =========================================================
# Time utils
# =========================================================
def now_taipei() -> datetime:
    return datetime.utcnow() + timedelta(hours=8)

# =========================================================
# Security utils
# =========================================================
def verify_line_signature(body: bytes, signature: str):
    if not LINE_CHANNEL_SECRET:
        logger.warning("âš ï¸ LINE_CHANNEL_SECRET æœªè¨­å®šï¼Œè·³éç°½åé©—è­‰")
        return
    mac = hmac.new(LINE_CHANNEL_SECRET.encode("utf-8"), body, hashlib.sha256).digest()
    expected = base64.b64encode(mac).decode("utf-8")
    if not hmac.compare_digest(signature.strip(), expected):
        logger.error("âŒ LINE ç°½åé©—è­‰å¤±æ•—")
        raise HTTPException(400, "Invalid Signature")

def require_admin(request: Request):
    if not ADMIN_TOKEN:
        raise HTTPException(500, "ADMIN_TOKEN not set")
    token = request.headers.get("X-Admin-Token", "")
    if not hmac.compare_digest(token, ADMIN_TOKEN):
        raise HTTPException(401, "Unauthorized")

def rate_limit_ok(user_id: str) -> bool:
    now = time.time()
    window_start = now - 60
    ts = RATE_STORE.get(user_id, [])
    ts = [t for t in ts if t >= window_start]
    if len(ts) >= RATE_LIMIT_PER_MIN:
        RATE_STORE[user_id] = ts
        return False
    ts.append(now)
    RATE_STORE[user_id] = ts
    return True

# =========================================================
# DB init
# =========================================================
def ensure_tables():
    """å»ºç«‹è³‡æ–™è¡¨å’Œç´¢å¼•"""
    logger.info("ğŸ”§ æª¢æŸ¥è³‡æ–™è¡¨...")
    
    with engine.begin() as conn:
        # Sales è¡¨
        conn.execute(text("""
        CREATE TABLE IF NOT EXISTS sales (
            date TEXT,
            customer TEXT,
            product TEXT,
            quantity REAL,
            amount REAL,
            year INTEGER
        );
        """))
        
        # Purchase è¡¨
        conn.execute(text("""
        CREATE TABLE IF NOT EXISTS purchase (
            date TEXT,
            supplier TEXT,
            product TEXT,
            quantity REAL,
            amount REAL,
            year INTEGER
        );
        """))
        
        # å˜—è©¦å»ºç«‹ç´¢å¼•ï¼ˆå¿½ç•¥éŒ¯èª¤ï¼‰
        try:
            conn.execute(text("CREATE INDEX IF NOT EXISTS idx_sales_date ON sales(date);"))
            conn.execute(text("CREATE INDEX IF NOT EXISTS idx_sales_year ON sales(year);"))
            conn.execute(text("CREATE INDEX IF NOT EXISTS idx_purchase_date ON purchase(date);"))
            conn.execute(text("CREATE INDEX IF NOT EXISTS idx_purchase_year ON purchase(year);"))
        except Exception as e:
            logger.warning(f"ç´¢å¼•å»ºç«‹å¤±æ•—ï¼ˆå¯å¿½ç•¥ï¼‰: {e}")
    
    logger.info("âœ… è³‡æ–™è¡¨æª¢æŸ¥å®Œæˆ")

def require_db_ready():
    global DB_READY
    if not DB_READY:
        ensure_tables()
        DB_READY = True

def table_counts() -> Dict[str, int]:
    """å–å¾—å„è¡¨ç­†æ•¸"""
    insp = inspect(engine)
    names = set(insp.get_table_names())
    out = {"sales": 0, "purchase": 0}
    
    with engine.connect() as conn:
        for t in out.keys():
            if t in names:
                try:
                    result = conn.execute(text(f"SELECT COUNT(*) FROM {t}"))
                    out[t] = int(result.scalar() or 0)
                except:
                    out[t] = 0
    
    return out

# =========================================================
# Google Sheet ä¸‹è¼‰ï¼ˆæ”¹é€²ç‰ˆï¼‰
# =========================================================
def extract_sheet_id(url: str) -> Optional[str]:
    """å¾å„ç¨® Google Sheet URL æ ¼å¼æå– ID"""
    patterns = [
        r"/spreadsheets/d/([a-zA-Z0-9_-]+)",
        r"id=([a-zA-Z0-9_-]+)",
    ]
    for pattern in patterns:
        m = re.search(pattern, url)
        if m:
            return m.group(1)
    return None

def download_google_sheet_xlsx(sheet_url: str, max_retries: int = 3) -> Optional[BytesIO]:
    """
    ä¸‹è¼‰ Google Sheet ç‚º Excel æ ¼å¼
    å›å‚³ BytesIO ç‰©ä»¶æˆ– Noneï¼ˆå¤±æ•—ï¼‰
    """
    sheet_id = extract_sheet_id(sheet_url)
    if not sheet_id:
        logger.error(f"âŒ ç„¡æ³•å¾ URL æå– Sheet ID: {sheet_url}")
        return None
    
    export_url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/export"
    params = {"format": "xlsx"}
    
    logger.info(f"ğŸ“¥ ä¸‹è¼‰ Google Sheet: {sheet_id}")
    
    for attempt in range(max_retries):
        try:
            response = requests.get(
                export_url,
                params=params,
                timeout=60,
                allow_redirects=True
            )
            
            # æª¢æŸ¥ç‹€æ…‹ç¢¼
            if response.status_code != 200:
                logger.error(f"âŒ ä¸‹è¼‰å¤±æ•—ï¼Œç‹€æ…‹ç¢¼: {response.status_code}")
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                return None
            
            # æª¢æŸ¥å…§å®¹é¡å‹
            content_type = response.headers.get('Content-Type', '')
            logger.info(f"Content-Type: {content_type}")
            
            # æª¢æŸ¥æ˜¯å¦ç‚º Excel æ ¼å¼
            content = response.content
            
            # Excel æª”æ¡ˆæ‡‰è©²ä»¥ PK é–‹é ­ï¼ˆZIP æ ¼å¼æ¨™è¨˜ï¼‰
            if not content.startswith(b'PK'):
                logger.error("âŒ å›æ‡‰ä¸æ˜¯ Excel æ ¼å¼")
                logger.error(f"å‰ 200 å­—å…ƒ: {content[:200]}")
                
                # å¦‚æœæ˜¯ HTMLï¼Œå¯èƒ½æ˜¯æ¬Šé™å•é¡Œ
                if b'<html' in content[:500].lower():
                    logger.error("âŒ æ”¶åˆ° HTML å›æ‡‰ï¼Œå¯èƒ½æ˜¯æ¬Šé™å•é¡Œ")
                    logger.error("è«‹ç¢ºèªï¼š")
                    logger.error("1. Google Sheet å·²è¨­å®šç‚ºã€ŒçŸ¥é“é€£çµçš„äººã€å¯ä»¥æª¢è¦–")
                    logger.error("2. URL æ ¼å¼æ­£ç¢º")
                    return None
                
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)
                    continue
                return None
            
            logger.info(f"âœ… ä¸‹è¼‰æˆåŠŸï¼Œå¤§å°: {len(content)} bytes")
            return BytesIO(content)
            
        except requests.exceptions.Timeout:
            logger.error(f"â±ï¸ ä¸‹è¼‰è¶…æ™‚ (å˜—è©¦ {attempt + 1}/{max_retries})")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
        except Exception as e:
            logger.error(f"âŒ ä¸‹è¼‰éŒ¯èª¤: {str(e)}")
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
    
    return None

# =========================================================
# ETL normalizeï¼ˆå½ˆæ€§ç‰ˆæœ¬ï¼‰
# =========================================================
def find_column(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:
    """å¾å€™é¸æ¬„ä½åç¨±ä¸­æ‰¾åˆ°ç¬¬ä¸€å€‹å­˜åœ¨çš„æ¬„ä½"""
    df.columns = df.columns.astype(str).str.strip()
    for candidate in candidates:
        for col in df.columns:
            if candidate in col:
                return col
    return None

def normalize_sales_df(df: pd.DataFrame) -> Optional[pd.DataFrame]:
    """æ¨™æº–åŒ–éŠ·å”®è³‡æ–™ï¼ˆå½ˆæ€§æ¬„ä½åŒ¹é…ï¼‰"""
    logger.info(f"è™•ç†éŠ·å”®è³‡æ–™ï¼Œæ¬„ä½: {list(df.columns)}")
    
    # å½ˆæ€§å°‹æ‰¾æ¬„ä½
    date_col = find_column(df, ["æ—¥æœŸ(è½‰æ›)", "æ—¥æœŸ", "Date", "date", "äº¤æ˜“æ—¥æœŸ"])
    customer_col = find_column(df, ["å®¢æˆ¶ä¾›æ‡‰å•†ç°¡ç¨±", "å®¢æˆ¶ç°¡ç¨±", "å®¢æˆ¶", "Customer", "å®¢æˆ¶åç¨±"])
    product_col = find_column(df, ["å“å", "ç”¢å“", "Product", "å“è™Ÿ", "ç”¢å“ä»£è™Ÿ"])
    quantity_col = find_column(df, ["æ•¸é‡", "Quantity", "éŠ·å”®æ•¸é‡"])
    amount_col = find_column(df, ["é€²éŠ·æ˜ç´°æœªç¨…é‡‘é¡", "æœªç¨…é‡‘é¡", "é‡‘é¡", "Amount", "å«ç¨…é‡‘é¡"])
    
    # æª¢æŸ¥å¿…è¦æ¬„ä½
    if not all([date_col, customer_col, product_col]):
        logger.warning(f"âš ï¸ ç¼ºå°‘å¿…è¦æ¬„ä½: date={date_col}, customer={customer_col}, product={product_col}")
        return None
    
    # å»ºç«‹æ¨™æº–åŒ– DataFrame
    clean = pd.DataFrame({
        "date": pd.to_datetime(df[date_col], errors="coerce"),
        "customer": df[customer_col].astype(str).str.strip(),
        "product": df[product_col].astype(str).str.strip(),
        "quantity": pd.to_numeric(df[quantity_col], errors="coerce").fillna(0) if quantity_col else 0,
        "amount": pd.to_numeric(df[amount_col], errors="coerce").fillna(0) if amount_col else 0,
    }).dropna(subset=["date"])
    
    clean["year"] = clean["date"].dt.year
    clean["date"] = clean["date"].dt.strftime("%Y-%m-%d")
    
    logger.info(f"âœ… æ¨™æº–åŒ–å®Œæˆï¼Œä¿ç•™ {len(clean)} ç­†è³‡æ–™")
    return clean

def normalize_purchase_df(df: pd.DataFrame) -> Optional[pd.DataFrame]:
    """æ¨™æº–åŒ–æ¡è³¼è³‡æ–™ï¼ˆå½ˆæ€§æ¬„ä½åŒ¹é…ï¼‰"""
    logger.info(f"è™•ç†æ¡è³¼è³‡æ–™ï¼Œæ¬„ä½: {list(df.columns)}")
    
    # å½ˆæ€§å°‹æ‰¾æ¬„ä½
    date_col = find_column(df, ["æ—¥æœŸ(è½‰æ›)", "æ—¥æœŸ", "Date", "date", "äº¤æ˜“æ—¥æœŸ"])
    supplier_col = find_column(df, ["å®¢æˆ¶ä¾›æ‡‰å•†ç°¡ç¨±", "ä¾›æ‡‰å•†", "Supplier", "å» å•†", "å» å•†åç¨±"])
    product_col = find_column(df, ["å°æ–¹å“å/å“åå‚™è¨»", "å“å", "ç”¢å“", "Product", "å“è™Ÿ"])
    quantity_col = find_column(df, ["æ•¸é‡", "Quantity", "æ¡è³¼æ•¸é‡"])
    amount_col = find_column(df, ["é€²éŠ·æ˜ç´°æœªç¨…é‡‘é¡", "æœªç¨…é‡‘é¡", "é‡‘é¡", "Amount", "å«ç¨…é‡‘é¡"])
    
    # æª¢æŸ¥å¿…è¦æ¬„ä½
    if not all([date_col, supplier_col, product_col]):
        logger.warning(f"âš ï¸ ç¼ºå°‘å¿…è¦æ¬„ä½: date={date_col}, supplier={supplier_col}, product={product_col}")
        return None
    
    # å»ºç«‹æ¨™æº–åŒ– DataFrame
    clean = pd.DataFrame({
        "date": pd.to_datetime(df[date_col], errors="coerce"),
        "supplier": df[supplier_col].astype(str).str.strip(),
        "product": df[product_col].astype(str).str.strip(),
        "quantity": pd.to_numeric(df[quantity_col], errors="coerce").fillna(0) if quantity_col else 0,
        "amount": pd.to_numeric(df[amount_col], errors="coerce").fillna(0) if amount_col else 0,
    }).dropna(subset=["date"])
    
    clean["year"] = clean["date"].dt.year
    clean["date"] = clean["date"].dt.strftime("%Y-%m-%d")
    
    logger.info(f"âœ… æ¨™æº–åŒ–å®Œæˆï¼Œä¿ç•™ {len(clean)} ç­†è³‡æ–™")
    return clean

# =========================================================
# Upsertï¼ˆæ’å…¥æˆ–å¿½ç•¥ï¼‰
# =========================================================
def bulk_insert(table: str, df: pd.DataFrame) -> int:
    """æ‰¹æ¬¡æ’å…¥è³‡æ–™"""
    if df.empty:
        return 0
    
    try:
        # ä½¿ç”¨ pandas ç›´æ¥å¯«å…¥
        rows_before = 0
        with engine.connect() as conn:
            result = conn.execute(text(f"SELECT COUNT(*) FROM {table}"))
            rows_before = result.scalar() or 0
        
        # å¯«å…¥è³‡æ–™ï¼ˆappend æ¨¡å¼ï¼‰
        df.to_sql(table, engine, if_exists='append', index=False)
        
        with engine.connect() as conn:
            result = conn.execute(text(f"SELECT COUNT(*) FROM {table}"))
            rows_after = result.scalar() or 0
        
        inserted = rows_after - rows_before
        logger.info(f"âœ… {table} å¯«å…¥å®Œæˆ: {inserted} ç­†æ–°è³‡æ–™")
        return inserted
        
    except Exception as e:
        logger.error(f"âŒ {table} å¯«å…¥å¤±æ•—: {str(e)}")
        return 0

def import_data_to_db() -> Dict[str, Any]:
    """å¾ Google Sheet åŒ¯å…¥è³‡æ–™"""
    require_db_ready()
    
    logger.info("ğŸ”„ é–‹å§‹è³‡æ–™åŒ¯å…¥...")
    t0 = time.time()
    before = table_counts()
    msgs = []
    inserted = {"sales": 0, "purchase": 0}
    
    # åŒ¯å…¥éŠ·å”®è³‡æ–™
    if SALES_SHEET_URL:
        logger.info("ğŸ“Š è™•ç†éŠ·å”®è³‡æ–™...")
        excel_bytes = download_google_sheet_xlsx(SALES_SHEET_URL)
        
        if excel_bytes:
            try:
                xls = pd.read_excel(excel_bytes, sheet_name=None)
                logger.info(f"æ‰¾åˆ° {len(xls)} å€‹å·¥ä½œè¡¨")
                
                dfs = []
                for sheet_name, df in xls.items():
                    logger.info(f"è™•ç†å·¥ä½œè¡¨: {sheet_name}")
                    normalized = normalize_sales_df(df)
                    if normalized is not None and len(normalized) > 0:
                        dfs.append(normalized)
                
                if dfs:
                    final = pd.concat(dfs, ignore_index=True)
                    # ç§»é™¤é‡è¤‡è³‡æ–™
                    final = final.drop_duplicates(subset=['date', 'customer', 'product', 'amount'], keep='first')
                    inserted["sales"] = bulk_insert("sales", final)
                    msgs.append(f"âœ… sales: è™•ç† {len(final)} ç­†ï¼Œæ–°å¢ {inserted['sales']} ç­†")
                else:
                    msgs.append("âš ï¸ sales: æ²’æœ‰ç¬¦åˆæ ¼å¼çš„å·¥ä½œè¡¨")
            except Exception as e:
                logger.error(f"âŒ sales è™•ç†éŒ¯èª¤: {str(e)}", exc_info=True)
                msgs.append(f"âŒ sales: è™•ç†å¤±æ•— - {str(e)}")
        else:
            msgs.append("âŒ sales: ä¸‹è¼‰å¤±æ•—")
    else:
        msgs.append("â„¹ï¸ sales: æœªè¨­å®š SALES_EXCEL_URL")
    
    # åŒ¯å…¥æ¡è³¼è³‡æ–™
    if PURCHASE_SHEET_URL:
        logger.info("ğŸ“¦ è™•ç†æ¡è³¼è³‡æ–™...")
        excel_bytes = download_google_sheet_xlsx(PURCHASE_SHEET_URL)
        
        if excel_bytes:
            try:
                xls = pd.read_excel(excel_bytes, sheet_name=None)
                logger.info(f"æ‰¾åˆ° {len(xls)} å€‹å·¥ä½œè¡¨")
                
                dfs = []
                for sheet_name, df in xls.items():
                    logger.info(f"è™•ç†å·¥ä½œè¡¨: {sheet_name}")
                    normalized = normalize_purchase_df(df)
                    if normalized is not None and len(normalized) > 0:
                        dfs.append(normalized)
                
                if dfs:
                    final = pd.concat(dfs, ignore_index=True)
                    # ç§»é™¤é‡è¤‡è³‡æ–™
                    final = final.drop_duplicates(subset=['date', 'supplier', 'product', 'amount'], keep='first')
                    inserted["purchase"] = bulk_insert("purchase", final)
                    msgs.append(f"âœ… purchase: è™•ç† {len(final)} ç­†ï¼Œæ–°å¢ {inserted['purchase']} ç­†")
                else:
                    msgs.append("âš ï¸ purchase: æ²’æœ‰ç¬¦åˆæ ¼å¼çš„å·¥ä½œè¡¨")
            except Exception as e:
                logger.error(f"âŒ purchase è™•ç†éŒ¯èª¤: {str(e)}", exc_info=True)
                msgs.append(f"âŒ purchase: è™•ç†å¤±æ•— - {str(e)}")
        else:
            msgs.append("âŒ purchase: ä¸‹è¼‰å¤±æ•—")
    else:
        msgs.append("â„¹ï¸ purchase: æœªè¨­å®š PURCHASE_EXCEL_URL")
    
    after = table_counts()
    cost = time.time() - t0
    
    result = {
        "ok": True,
        "before": before,
        "after": after,
        "inserted": inserted,
        "seconds": round(cost, 2),
        "messages": msgs
    }
    
    logger.info(f"âœ¨ è³‡æ–™åŒ¯å…¥å®Œæˆ: {result}")
    return result

# =========================================================
# AI æŸ¥è©¢è™•ç†ï¼ˆGemini Function Callingï¼‰
# =========================================================
def execute_sql_query(sql: str) -> str:
    """åŸ·è¡Œ SQL æŸ¥è©¢"""
    logger.info(f"ğŸ” åŸ·è¡Œ SQL: {sql[:200]}")
    
    if not sql.strip().lower().startswith("select"):
        return "éŒ¯èª¤ï¼šåªå…è¨± SELECT æŸ¥è©¢"
    
    try:
        with engine.connect() as conn:
            df = pd.read_sql(text(sql), conn)
            if df.empty:
                return "æŸ¥è©¢æˆåŠŸä½†ç„¡è³‡æ–™"
            
            # é™åˆ¶ç­†æ•¸
            if len(df) > 50:
                df = df.head(50)
            
            return df.to_json(orient="records", force_ascii=False)
    except Exception as e:
        logger.error(f"âŒ SQL éŒ¯èª¤: {str(e)}")
        return f"SQL éŒ¯èª¤: {str(e)}"

def create_chart(title: str, chart_type: str, data_json: str, x_key: str, y_key: str) -> str:
    """ç”Ÿæˆåœ–è¡¨"""
    logger.info(f"ğŸ“Š ç¹ªè£½åœ–è¡¨: {title} ({chart_type})")
    
    try:
        data = json.loads(data_json)
        df = pd.DataFrame(data)
        
        if df.empty:
            return "ç„¡è³‡æ–™ç¹ªåœ–"
        
        plt.figure(figsize=(10, 6))
        plt.rcParams['font.sans-serif'] = ['Arial Unicode MS', 'Microsoft JhengHei']
        plt.rcParams['axes.unicode_minus'] = False
        
        df[y_key] = pd.to_numeric(df[y_key], errors='coerce').fillna(0)
        
        if chart_type == "line":
            plt.plot(df[x_key], df[y_key], marker='o', linewidth=2)
        elif chart_type == "bar":
            plt.bar(df[x_key], df[y_key])
            plt.xticks(rotation=45, ha='right')
        elif chart_type == "pie":
            plt.pie(df[y_key], labels=df[x_key], autopct='%1.1f%%')
        
        plt.title(title)
        plt.tight_layout()
        
        buf = BytesIO()
        plt.savefig(buf, format="png", dpi=100)
        plt.close()
        
        img_id = f"chart_{int(time.time())}_{hash(title) % 10000}"
        IMG_STORE[img_id] = {"bytes": buf.getvalue(), "ts": time.time()}
        
        logger.info(f"âœ… åœ–è¡¨ç”Ÿæˆ: {img_id}")
        return f"IMAGE_ID:{img_id}"
        
    except Exception as e:
        logger.error(f"âŒ åœ–è¡¨éŒ¯èª¤: {str(e)}")
        return f"åœ–è¡¨éŒ¯èª¤: {str(e)}"

# Gemini å·¥å…·å®šç¾©
tools_list = [execute_sql_query, create_chart]
google_search = {"google_search": {}}

# ç³»çµ±æç¤ºè©
SYSTEM_PROMPT = """ä½ æ˜¯å°ˆæ¥­çš„ ERP è³‡æ–™åˆ†æåŠ©ç†ã€Œå°æ™ºã€ã€‚

## ğŸ“Š å¯ç”¨è³‡æ–™è¡¨
- **sales** (éŠ·å”®): date, customer, product, quantity, amount, year
- **purchase** (æ¡è³¼): date, supplier, product, quantity, amount, year

## ğŸ¯ ä½ çš„ä»»å‹™
1. ç†è§£ç”¨æˆ¶å•é¡Œ
2. ä½¿ç”¨ execute_sql_query æŸ¥è©¢è³‡æ–™
3. ç”¨ create_chart è¦–è¦ºåŒ–ï¼ˆéœ€è¦æ™‚ï¼‰
4. æä¾›å°ˆæ¥­åˆ†æå’Œå»ºè­°

## ğŸ’¡ æŸ¥è©¢ç¯„ä¾‹
```sql
-- 2024å¹´ç¸½éŠ·å”®é¡
SELECT SUM(amount) as total FROM sales WHERE year = 2024;

-- å‰10å¤§å®¢æˆ¶
SELECT customer, SUM(amount) as total 
FROM sales 
GROUP BY customer 
ORDER BY total DESC 
LIMIT 10;
```

## ğŸ“ˆ ç¹ªåœ–æµç¨‹
1. å…ˆç”¨ SQL æŸ¥è©¢è³‡æ–™
2. å°‡æŸ¥è©¢çµæœçš„ JSON å‚³çµ¦ create_chart
3. chart_type: "line"(è¶¨å‹¢), "bar"(æ¯”è¼ƒ), "pie"(ä½”æ¯”)

## åŸå‰‡
- ä¸»å‹•æä¾›æ´å¯Ÿï¼Œä¸åªå›ç­”å•é¡Œ
- æ•¸æ“šèªªè©±ï¼Œç”¨å¯¦éš›æ•¸å­—æ”¯æŒè§€é»
- å»ºè­°è¦–è¦ºåŒ–æ™‚ä¸»å‹•ç¹ªåœ–
- ç”¨ç¹é«”ä¸­æ–‡ï¼Œå‹å–„å°ˆæ¥­"""

async def agent_process(user_id: str, text: str, base_url: str) -> Dict[str, Any]:
    """AI Agent è™•ç†ç”¨æˆ¶è¨Šæ¯"""
    if not client:
        return {"text": "âŒ Gemini API æœªè¨­å®š"}
    
    logger.info(f"ğŸ¤– è™•ç†è¨Šæ¯: {text}")
    
    history = CHAT_MEMORY.get(user_id, [])
    
    try:
        # æ§‹å»ºå°è©±
        contents = history + [text]
        
        config = types.GenerateContentConfig(
            tools=tools_list + [google_search],
            system_instruction=SYSTEM_PROMPT,
            temperature=0.7
        )
        
        final_text = ""
        image_url = None
        max_turns = 5
        
        for turn in range(max_turns):
            logger.info(f"ğŸ”„ ç¬¬ {turn + 1} è¼ªè™•ç†")
            
            response = client.models.generate_content(
                model=MODEL_NAME,
                contents=contents,
                config=config
            )
            
            if not response.candidates:
                final_text = "æŠ±æ­‰ï¼Œç„¡æ³•è™•ç†æ­¤è«‹æ±‚"
                break
            
            candidate = response.candidates[0]
            content = candidate.content
            
            # æª¢æŸ¥å·¥å…·èª¿ç”¨
            has_tool = any(hasattr(p, 'function_call') for p in content.parts)
            
            if has_tool:
                # è™•ç†å·¥å…·èª¿ç”¨
                tool_responses = []
                
                for part in content.parts:
                    if not hasattr(part, 'function_call'):
                        continue
                    
                    fc = part.function_call
                    logger.info(f"ğŸ”§ èª¿ç”¨å·¥å…·: {fc.name}")
                    
                    if fc.name == "execute_sql_query":
                        result = execute_sql_query(fc.args.get("sql", ""))
                    elif fc.name == "create_chart":
                        result = create_chart(
                            fc.args.get("title", "åœ–è¡¨"),
                            fc.args.get("chart_type", "bar"),
                            fc.args.get("data_json", "[]"),
                            fc.args.get("x_key", ""),
                            fc.args.get("y_key", "")
                        )
                        if "IMAGE_ID:" in result:
                            img_id = result.split(":")[1]
                            image_url = f"{base_url}/img/{img_id}"
                            result = "åœ–è¡¨å·²ç”Ÿæˆ"
                    else:
                        result = f"æœªçŸ¥å·¥å…·: {fc.name}"
                    
                    tool_responses.append(
                        types.Part(
                            function_response=types.FunctionResponse(
                                name=fc.name,
                                response={"result": result}
                            )
                        )
                    )
                
                # åŠ å…¥å°è©±
                contents.append(content)
                contents.append(types.Content(role="user", parts=tool_responses))
            else:
                # æ²’æœ‰å·¥å…·èª¿ç”¨ï¼Œå–å¾—æœ€çµ‚å›æ‡‰
                final_text = response.text
                break
        
        # æ›´æ–°è¨˜æ†¶
        CHAT_MEMORY[user_id] = contents[-10:]
        
        return {
            "text": final_text or "è™•ç†å®Œæˆ",
            "image": image_url
        }
        
    except Exception as e:
        logger.error(f"âŒ Agent éŒ¯èª¤: {str(e)}", exc_info=True)
        return {"text": f"ç³»çµ±éŒ¯èª¤: {str(e)}"}

# =========================================================
# LINE å›è¦†
# =========================================================
async def reply_line(reply_token: str, text_out: Optional[str], img_url: Optional[str] = None):
    """å›è¦† LINE è¨Šæ¯"""
    if not LINE_CHANNEL_ACCESS_TOKEN:
        logger.warning("âš ï¸ LINE_CHANNEL_ACCESS_TOKEN æœªè¨­å®š")
        return
    
    headers = {
        "Authorization": f"Bearer {LINE_CHANNEL_ACCESS_TOKEN}",
        "Content-Type": "application/json"
    }
    
    messages = []
    
    if img_url:
        messages.append({
            "type": "image",
            "originalContentUrl": img_url,
            "previewImageUrl": img_url
        })
    
    if text_out:
        messages.append({
            "type": "text",
            "text": text_out[:4999]
        })
    
    if not messages:
        messages.append({"type": "text", "text": "è™•ç†å®Œæˆ"})
    
    payload = {
        "replyToken": reply_token,
        "messages": messages
    }
    
    try:
        async with httpx.AsyncClient(timeout=20) as c:
            response = await c.post(
                "https://api.line.me/v2/bot/message/reply",
                headers=headers,
                json=payload
            )
            if response.status_code == 200:
                logger.info("âœ… LINE è¨Šæ¯å·²é€å‡º")
            else:
                logger.error(f"âŒ LINE API éŒ¯èª¤: {response.status_code} - {response.text}")
    except Exception as e:
        logger.error(f"âŒ ç™¼é€è¨Šæ¯å¤±æ•—: {str(e)}")

# =========================================================
# Routes
# =========================================================
@app.get("/")
def root():
    """æ ¹è·¯å¾‘"""
    return {
        "status": "ok",
        "service": "ERP Bot Ultra PRO",
        "version": "3.2",
        "timestamp": now_taipei().isoformat()
    }

@app.get("/health")
def health():
    """å¥åº·æª¢æŸ¥"""
    require_db_ready()
    
    checks = {
        "database": False,
        "gemini": bool(client),
        "line": bool(LINE_CHANNEL_ACCESS_TOKEN),
        "sales_url": bool(SALES_SHEET_URL),
        "purchase_url": bool(PURCHASE_SHEET_URL)
    }
    
    try:
        with engine.connect() as conn:
            conn.execute(text("SELECT 1"))
        checks["database"] = True
    except:
        pass
    
    counts = table_counts()
    
    return {
        "status": "healthy" if checks["database"] and checks["gemini"] and checks["line"] else "degraded",
        "checks": checks,
        "counts": counts,
        "timestamp": now_taipei().isoformat()
    }

@app.get("/img/{img_id}")
def get_img(img_id: str):
    """å–å¾—åœ–ç‰‡"""
    if img_id not in IMG_STORE:
        raise HTTPException(404, "åœ–ç‰‡ä¸å­˜åœ¨")
    
    return Response(
        content=IMG_STORE[img_id]["bytes"],
        media_type="image/png",
        headers={"Cache-Control": "public, max-age=3600"}
    )

@app.post("/admin/reload_sync")
def admin_reload_sync(request: Request):
    """ç®¡ç†å“¡ï¼šæ‰‹å‹•åŒæ­¥è³‡æ–™"""
    require_admin(request)
    return import_data_to_db()

@app.post("/admin/clear_data")
def admin_clear_data(request: Request):
    """ç®¡ç†å“¡ï¼šæ¸…ç©ºè³‡æ–™è¡¨"""
    require_admin(request)
    
    with engine.begin() as conn:
        conn.execute(text("DELETE FROM sales"))
        conn.execute(text("DELETE FROM purchase"))
    
    return {"ok": True, "message": "è³‡æ–™å·²æ¸…ç©º"}

@app.post("/line/webhook")
async def webhook(request: Request, background_tasks: BackgroundTasks):
    """LINE Webhook"""
    body = await request.body()
    signature = request.headers.get("X-Line-Signature", "")
    
    # é©—è­‰ç°½å
    verify_line_signature(body, signature)
    
    # è§£æäº‹ä»¶
    try:
        events = json.loads(body.decode("utf-8")).get("events", [])
    except Exception as e:
        logger.error(f"âŒ JSON è§£æå¤±æ•—: {str(e)}")
        return {"ok": False}
    
    base_url = f"https://{request.headers.get('host', 'localhost')}"
    
    for event in events:
        event_type = event.get("type")
        logger.info(f"ğŸ“¨ æ”¶åˆ°äº‹ä»¶: {event_type}")
        
        # è¨Šæ¯äº‹ä»¶
        if event_type == "message":
            message = event.get("message", {})
            
            if message.get("type") == "text":
                user_id = event["source"]["userId"]
                user_text = message["text"]
                reply_token = event["replyToken"]
                
                logger.info(f"ğŸ‘¤ ç”¨æˆ¶ {user_id}: {user_text}")
                
                # æª¢æŸ¥é™æµ
                if not rate_limit_ok(user_id):
                    background_tasks.add_task(
                        reply_line,
                        reply_token,
                        "è«‹ç¨å¾Œå†è©¦ï¼ˆè«‹æ±‚éæ–¼é »ç¹ï¼‰",
                        None
                    )
                    continue
                
                # ç‰¹æ®ŠæŒ‡ä»¤
                if user_text.strip().lower() in ["/reset", "/æ¸…é™¤", "æ¸…é™¤"]:
                    CHAT_MEMORY.pop(user_id, None)
                    background_tasks.add_task(
                        reply_line,
                        reply_token,
                        "âœ… å°è©±è¨˜æ†¶å·²æ¸…é™¤",
                        None
                    )
                    continue
                
                if user_text.strip().lower() in ["/help", "/èªªæ˜", "èªªæ˜"]:
                    help_text = """ğŸ¤– ERP åŠ©ç†ä½¿ç”¨èªªæ˜

ğŸ“Š **æŸ¥è©¢åŠŸèƒ½**
â€¢ ç›´æ¥å•å•é¡Œï¼Œä¾‹å¦‚ï¼š
  - 2024å¹´ç¸½éŠ·å”®é¡ï¼Ÿ
  - å‰10å¤§å®¢æˆ¶æ˜¯èª°ï¼Ÿ
  - æ¡è³¼è¶¨å‹¢å¦‚ä½•ï¼Ÿ

ğŸ“ˆ **è¦–è¦ºåŒ–åŠŸèƒ½**
â€¢ è¦æ±‚ç¹ªåœ–ï¼Œä¾‹å¦‚ï¼š
  - ç•«å‡ºæœˆéŠ·å”®è¶¨å‹¢
  - é¡¯ç¤ºç”¢å“ä½”æ¯”
  - æ¯”è¼ƒå„å¹´åº¦æ¥­ç¸¾

âš™ï¸ **æŒ‡ä»¤**
/æ¸…é™¤ - æ¸…é™¤å°è©±è¨˜æ†¶
/èªªæ˜ - é¡¯ç¤ºæ­¤èªªæ˜

ğŸ’¡ **æç¤º**
æˆ‘æœƒä¸»å‹•æä¾›åˆ†æå’Œå»ºè­°ï¼"""
                    background_tasks.add_task(
                        reply_line,
                        reply_token,
                        help_text,
                        None
                    )
                    continue
                
                # AI è™•ç†
                background_tasks.add_task(
                    handle_message,
                    user_id,
                    user_text,
                    reply_token,
                    base_url
                )
        
        # è¿½è¹¤äº‹ä»¶ï¼ˆåŠ å…¥å¥½å‹ï¼‰
        elif event_type == "follow":
            reply_token = event["replyToken"]
            welcome = """ğŸ‘‹ æ­¡è¿ä½¿ç”¨ ERP æ™ºèƒ½åŠ©ç†ï¼

æˆ‘å¯ä»¥å¹«ä½ ï¼š
ğŸ“Š æŸ¥è©¢éŠ·å”®å’Œæ¡è³¼æ•¸æ“š
ğŸ“ˆ ç”Ÿæˆåœ–è¡¨å’Œè¶¨å‹¢åˆ†æ
ğŸ’¡ æä¾›å•†æ¥­æ´å¯Ÿ

è©¦è©¦å•æˆ‘ï¼š
â€¢ ã€Œ2024å¹´ç¸½éŠ·å”®é¡ï¼Ÿã€
â€¢ ã€Œå‰10å¤§å®¢æˆ¶ï¼Ÿã€
â€¢ ã€Œç•«å‡ºéŠ·å”®è¶¨å‹¢åœ–ã€

è¼¸å…¥ /èªªæ˜ æŸ¥çœ‹æ›´å¤šåŠŸèƒ½ï¼"""
            
            background_tasks.add_task(reply_line, reply_token, welcome, None)
    
    return {"ok": True}

async def handle_message(user_id: str, user_text: str, reply_token: str, base_url: str):
    """è™•ç†è¨Šæ¯ï¼ˆèƒŒæ™¯ä»»å‹™ï¼‰"""
    try:
        # æª¢æŸ¥è³‡æ–™åº«æ˜¯å¦æœ‰è³‡æ–™
        counts = table_counts()
        if counts["sales"] == 0 and counts["purchase"] == 0:
            await reply_line(
                reply_token,
                "âš ï¸ è³‡æ–™åº«ç›®å‰æ²’æœ‰è³‡æ–™\n\nè«‹ç®¡ç†å“¡å…ˆåŒæ­¥ Google Sheet è³‡æ–™ã€‚",
                None
            )
            return
        
        # AI è™•ç†
        result = await agent_process(user_id, user_text, base_url)
        await reply_line(reply_token, result.get("text"), result.get("image"))
        
    except Exception as e:
        logger.error(f"âŒ è™•ç†è¨Šæ¯éŒ¯èª¤: {str(e)}", exc_info=True)
        await reply_line(reply_token, f"ç³»çµ±éŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦", None)

# =========================================================
# å•Ÿå‹•äº‹ä»¶
# =========================================================
@app.on_event("startup")
async def startup():
    """æ‡‰ç”¨å•Ÿå‹•"""
    global DB_READY
    
    logger.info("ğŸš€ æ‡‰ç”¨å•Ÿå‹•ä¸­...")
    
    # æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
    logger.info(f"DATABASE_URL: {'âœ…' if DATABASE_URL else 'âŒ'}")
    logger.info(f"GEMINI_API_KEY: {'âœ…' if GEMINI_API_KEY else 'âŒ'}")
    logger.info(f"LINE_CHANNEL_ACCESS_TOKEN: {'âœ…' if LINE_CHANNEL_ACCESS_TOKEN else 'âŒ'}")
    logger.info(f"LINE_CHANNEL_SECRET: {'âœ…' if LINE_CHANNEL_SECRET else 'âŒ'}")
    logger.info(f"SALES_EXCEL_URL: {'âœ…' if SALES_SHEET_URL else 'âŒ'}")
    logger.info(f"PURCHASE_EXCEL_URL: {'âœ…' if PURCHASE_SHEET_URL else 'âŒ'}")
    
    # å»ºç«‹è³‡æ–™è¡¨
    try:
        ensure_tables()
        DB_READY = True
        logger.info("âœ… è³‡æ–™åº«åˆå§‹åŒ–å®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—: {str(e)}")
    
    # è‡ªå‹•åŒ¯å…¥
    if AUTO_IMPORT_ON_STARTUP:
        logger.info("ğŸ”„ å•Ÿå‹•æ™‚è‡ªå‹•åŒ¯å…¥è³‡æ–™...")
        try:
            result = import_data_to_db()
            logger.info(f"âœ… è‡ªå‹•åŒ¯å…¥å®Œæˆ: {result}")
        except Exception as e:
            logger.error(f"âŒ è‡ªå‹•åŒ¯å…¥å¤±æ•—: {str(e)}")
    
    logger.info("âœ¨ æ‡‰ç”¨å•Ÿå‹•å®Œæˆï¼")

@app.on_event("shutdown")
async def shutdown():
    """æ‡‰ç”¨é—œé–‰"""
    logger.info("ğŸ‘‹ æ‡‰ç”¨é—œé–‰ä¸­...")
    IMG_STORE.clear()
    CHAT_MEMORY.clear()
    logger.info("âœ… æ¸…ç†å®Œæˆ")